{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G3F1CgH6GFmP"
      },
      "outputs": [],
      "source": [
        "# Reproducing the Paper step 1:\n",
        "# Training a Vanilla Transformer on CIFAR-10.\n",
        "# It is easier to visualize Attention scores on images, far more intuitive than text data.\n",
        "# We load CIFAR-10 as pixel sequences 32x32x3 = 3072 bytes per image\n",
        "# Text would require byte pair encoding, we already have raw bytes, just need to add an embedding layer\n",
        "# and positional encoding suitable to the data dimensions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A8fS2UOPpmL",
        "outputId": "061c4196-1cfa-4fe4-ccd0-c515b4424b99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-25 16:00:34--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf cifar-10-python.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTaXLzBoS5rw",
        "outputId": "67cb44d4-31cc-425a-af3f-2dbfbd271a11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as cPickle\n",
        "import numpy as np\n",
        "import random\n",
        "from pathlib import Path\n",
        "import os\n",
        "random.seed(1) # set a seed so that the results are consistent\n",
        "\n",
        "def load_train():\n",
        "    path = Path('./cifar-10-batches-py')\n",
        "    filestring = 'data_batch_'\n",
        "    contents = [path / x for x in os.listdir(path) if filestring in x]\n",
        "\n",
        "    images = [cPickle.load(open(f, \"rb\"), encoding=\"latin1\") for f in contents]\n",
        "\n",
        "    images = [x[\"data\"] for x in images]\n",
        "    imagearray = np.array(images)   #   (5, 10000, 3072)\n",
        "    return np.vstack(imagearray) # (50000, 3072)\n",
        "\n",
        "def load_test():\n",
        "    path = Path('./cifar-10-batches-py')\n",
        "    filestring = 'test_batch'\n",
        "    contents = [path / x for x in os.listdir(path) if filestring in x]\n",
        "\n",
        "    images = [cPickle.load(open(f, \"rb\"), encoding=\"latin1\") for f in contents]\n",
        "\n",
        "    images = [x[\"data\"] for x in images]\n",
        "    imagearray = np.array(images)   #   (5, 10000, 3072)\n",
        "    return np.vstack(imagearray) # (50000, 3072)\n"
      ],
      "metadata": {
        "id": "imPYK-CYT2zN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = load_train()\n",
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBFNkfowUHYb",
        "outputId": "03bbd443-8acb-41ff-97c5-e170575c0412"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 3072)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = load_test()\n",
        "imgs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIqY0LU8lAKw",
        "outputId": "00ee452d-4b21-436f-f68b-ef28a5178553"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3072)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's save one of the images to disk to verify integrity\n",
        "images = np.reshape(images, (50000, 3, 32, 32))\n",
        "img = images[1000]\n",
        "img = np.transpose(img, (1, 2, 0))\n",
        "img.shape\n",
        "import matplotlib\n",
        "matplotlib.pyplot.imsave('img.jpg', img)     # image no #"
      ],
      "metadata": {
        "id": "dOZyrFeHXVyP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = load_train()\n",
        "bytesequence = images[0]\n",
        "bytesequence[0:100]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKK03cL8UE3m",
        "outputId": "e230d941-0b0b-4fe2-d77f-a81524bd115e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 59,  43,  50,  68,  98, 119, 139, 145, 149, 149, 131, 125, 142,\n",
              "       144, 137, 129, 137, 134, 124, 139, 139, 133, 136, 139, 152, 163,\n",
              "       168, 159, 158, 158, 152, 148,  16,   0,  18,  51,  88, 120, 128,\n",
              "       127, 126, 116, 106, 101, 105, 113, 109, 112, 119, 109, 105, 125,\n",
              "       127, 122, 131, 124, 121, 131, 132, 133, 133, 123, 119, 122,  25,\n",
              "        16,  49,  83, 110, 129, 130, 121, 113, 112, 112, 106, 105, 128,\n",
              "       124, 130, 127, 122, 115, 120, 130, 131, 139, 127, 126, 127, 130,\n",
              "       142, 130, 118, 120, 109,  33,  38,  87, 106], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's implement our vanilla attention baseline\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "0hOPIyA834CQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  \"\"\"LayerNorm with optional bias\"\"\"\n",
        "  def __init__(self, ndim, bias):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "  def forward(self, input):\n",
        "    return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n"
      ],
      "metadata": {
        "id": "lYmp_yez4RSI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    # key, query, value projections for all heads, but in a batch\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "    # output projection\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "    # regularization\n",
        "    self.attn_dropout = nn.Dropout(config.dropout)\n",
        "    self.resid_dropout = nn.Dropout(config.dropout)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.dropout = config.dropout\n",
        "    # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()\n",
        "    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "    q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    att = self.attn_dropout(att)\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.resid_dropout(self.c_proj(y))\n",
        "    return y"
      ],
      "metadata": {
        "id": "TrDKszBz46R4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7ONXRImt6PLw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "JaRDCZiS6Tey"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 3072\n",
        "    vocab_size: int = 256 # 256 possible byte values\n",
        "    n_layer: int = 128\n",
        "    n_head: int = 2\n",
        "    n_embd: int = 256\n",
        "    dropout: float = 0.25\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n"
      ],
      "metadata": {
        "id": "Gxa3o4ti6Vpz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:  mlp and query-key projections to half-size"
      ],
      "metadata": {
        "id": "ivaefOGX7YGx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        pass\n",
        "        # TODO: Implement for easy checkpointing\n",
        "        # return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 8e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS # 8.1 for the T4 in colab\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Zhbywm7i7jqi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now it's time to convert our training and validation data to .bin files and\n",
        "# set up the training loop"
      ],
      "metadata": {
        "id": "93LrVJid99-o"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we save the train and test data to .bin files to conform with the rest of the transformer implementation\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "train_data = load_train()\n",
        "val_data = load_test()\n",
        "# data is already encoded as raw bytes\n",
        "\n",
        "# export to bin files\n",
        "\n",
        "train_data.tofile('train.bin')\n",
        "val_data.tofile('val.bin')\n"
      ],
      "metadata": {
        "id": "G9qB6kTX-e-5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, val_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQdsmEkElfzb",
        "outputId": "4fd01423-32a2-4cdd-81b9-da32208d1647"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 3072), (10000, 3072))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir out/"
      ],
      "metadata": {
        "id": "mjFKZLaJnn5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e59b30f-db4c-4b0a-a7ed-fd513f82e596"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘out/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group"
      ],
      "metadata": {
        "id": "SH48eP79mrKf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 2000\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "\n",
        "# TODO: set up wandb logging\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'sparse-transformer'\n",
        "wandb_run_name = 'cifar-10' # 'run' + str(time.time())\n",
        "\n",
        "# data\n",
        "dataset = 'cifar-10'\n",
        "gradient_accumulation_steps = 5 * 6 # used to simulate larger batch sizes\n",
        "batch_size = 6 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 3072\n",
        "\n",
        "# model\n",
        "# For testing purposes\n",
        "n_layer = 12\n",
        "n_head = 2\n",
        "n_embd = 128\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "\n",
        "# for actual training:\n",
        "# block_size: int = 3072\n",
        "# vocab_size: int = 256 # 256 possible byte values\n",
        "# n_layer: int = 128\n",
        "# n_head: int = 2\n",
        "# n_embd: int = 256\n",
        "# dropout: float = 0.25\n",
        "# bias: bool = True\n",
        "\n",
        "# adamw optimizer\n",
        "learning_rate = 0.00035 # max learning rate\n",
        "# 120 epochs of 50k images = 6 million iterations / batch size = 500k iters\n",
        "max_iters = 500000 # total number of training iterations\n",
        "weight_decay = 0.01\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "# gradient clipping as specified in paper\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 5000 # how many steps to warm up for\n",
        "lr_decay_iters = 500000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 0.000035 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster"
      ],
      "metadata": {
        "id": "BhZy4_7Nn9mV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "# TODO: implement distributed training"
      ],
      "metadata": {
        "id": "CdzXH9WIshFf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if ddp:\n",
        "    pass\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1"
      ],
      "metadata": {
        "id": "3qrxBPQIsoLB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLZ5nHxEswms",
        "outputId": "339b3938-6dc1-440f-c400-fed71b870141"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 552,960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "iRbWR67Us9Km"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"data_dir\", exist_ok=True)"
      ],
      "metadata": {
        "id": "UWRWgNh2tD9g"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv train.bin data_dir/\n",
        "!mv val.bin data_dir/"
      ],
      "metadata": {
        "id": "tSwg8OsZtL0x"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(\"data_dir\", 'train.bin'), dtype=np.uint8, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(\"data_dir\", 'val.bin'), dtype=np.uint8, mode='r')\n",
        "    # divide by block_size since we treat images as discrete samples\n",
        "    ix = torch.randint(len(data)//block_size - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "OfVt_DcutkVo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.memmap(os.path.join(\"data_dir\", 'train.bin'), dtype=np.uint8, mode='r')\n",
        "data.shape\n",
        "# data is written to disk as one dimensional array of bytes. We need to sample\n",
        "# chunks with offsets 3072*n\n",
        "ix = torch.randint(len(data)//block_size - block_size, (batch_size,))\n",
        "x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE5hPjXbt2vC",
        "outputId": "0e62ef80-476e-4b15-9f02-a6102014d5bc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: technically we need to remove one byte. how much does this influence the\n",
        "y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn8EJ4WivREZ",
        "outputId": "33f75669-8d6a-43dc-91eb-04c95a89ae60"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = get_batch(\"train\")\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "huH5lDfyt-4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea0758e-a6af-42ba-b1ad-b575047ffcc4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6, 3072]), torch.Size([6, 3072]))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "meta_vocab_size = 256"
      ],
      "metadata": {
        "id": "mUA9Os5bu2cy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line"
      ],
      "metadata": {
        "id": "pvRXjIyzwyoc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 256\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hDCC5F3w6Bm",
        "outputId": "f626b1e2-cf91-4093-be7d-52161d292d20"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 2.40M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "b-sJgzXxxDii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9189eb-2630-4abe-a3ff-69784978ccb4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(256, 128)\n",
              "    (wpe): Embedding(3072, 128)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=128, out_features=384, bias=False)\n",
              "          (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=128, out_features=512, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=128, out_features=256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "# TODO: implement model.configure_optimizers\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory\n"
      ],
      "metadata": {
        "id": "O3W_d_9xxJXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca724ff-c58f-4b8e-829d-e869e931e2e3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1077376186.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 2,785,280 parameters\n",
            "num non-decayed parameter tensors: 25, with 3,200 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n"
      ],
      "metadata": {
        "id": "sHvV4SD4xXIq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1eb6f04-b7bf-4c1a-c40e-96e260aaed7d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "O2GOrCUMxexV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * (it + 1) / (warmup_iters + 1)\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "ITiX7150xhiA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n"
      ],
      "metadata": {
        "id": "Wbi1hnhdxnid"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "id": "2U4YEeO8xr8p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de0f7452-5d84-407f-d8cf-b06159d1c4d1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0825 16:01:05.689000 7918 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 5.4896, val loss 5.4795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0: loss 5.5220, time 150345.21ms, mfu -100.00%\n",
            "iter 1: loss 5.4898, time 30592.17ms, mfu -100.00%\n",
            "iter 2: loss 5.4871, time 31544.12ms, mfu -100.00%\n",
            "iter 3: loss 5.5165, time 31627.04ms, mfu -100.00%\n",
            "iter 4: loss 5.4761, time 31624.82ms, mfu -100.00%\n",
            "iter 5: loss 5.4616, time 31583.90ms, mfu 15.54%\n",
            "iter 6: loss 5.4342, time 31596.66ms, mfu 15.54%\n",
            "iter 7: loss 5.5058, time 31582.66ms, mfu 15.54%\n",
            "iter 8: loss 5.4893, time 31601.04ms, mfu 15.54%\n",
            "iter 9: loss 5.4692, time 31584.23ms, mfu 15.54%\n",
            "iter 10: loss 5.4486, time 31605.29ms, mfu 15.53%\n",
            "iter 11: loss 5.4758, time 31592.01ms, mfu 15.53%\n",
            "iter 12: loss 5.5010, time 31597.69ms, mfu 15.53%\n",
            "iter 13: loss 5.5007, time 31585.77ms, mfu 15.53%\n",
            "iter 14: loss 5.5086, time 31619.57ms, mfu 15.53%\n",
            "iter 15: loss 5.5292, time 31594.28ms, mfu 15.53%\n",
            "iter 16: loss 5.4659, time 31611.25ms, mfu 15.53%\n",
            "iter 17: loss 5.4447, time 31609.25ms, mfu 15.53%\n",
            "iter 18: loss 5.4539, time 31621.87ms, mfu 15.53%\n",
            "iter 19: loss 5.4858, time 31577.14ms, mfu 15.53%\n",
            "iter 20: loss 5.5043, time 31591.65ms, mfu 15.53%\n",
            "iter 21: loss 5.4457, time 31592.73ms, mfu 15.53%\n",
            "iter 22: loss 5.4829, time 31589.07ms, mfu 15.53%\n",
            "iter 23: loss 5.4912, time 31584.18ms, mfu 15.53%\n",
            "iter 24: loss 5.4564, time 31590.82ms, mfu 15.53%\n",
            "iter 25: loss 5.4906, time 31605.91ms, mfu 15.53%\n",
            "iter 26: loss 5.4912, time 31587.78ms, mfu 15.53%\n",
            "iter 27: loss 5.4806, time 31618.47ms, mfu 15.53%\n",
            "iter 28: loss 5.4339, time 31588.25ms, mfu 15.53%\n",
            "iter 29: loss 5.4898, time 31611.55ms, mfu 15.53%\n",
            "iter 30: loss 5.4813, time 31595.19ms, mfu 15.53%\n",
            "iter 31: loss 5.4670, time 31608.39ms, mfu 15.53%\n",
            "iter 32: loss 5.4549, time 31593.21ms, mfu 15.53%\n",
            "iter 33: loss 5.4960, time 31610.04ms, mfu 15.53%\n",
            "iter 34: loss 5.4812, time 31592.81ms, mfu 15.53%\n",
            "iter 35: loss 5.4664, time 31599.38ms, mfu 15.53%\n",
            "iter 36: loss 5.4829, time 31578.61ms, mfu 15.53%\n",
            "iter 37: loss 5.4519, time 31616.54ms, mfu 15.53%\n",
            "iter 38: loss 5.4696, time 31588.14ms, mfu 15.53%\n",
            "iter 39: loss 5.4782, time 31582.15ms, mfu 15.53%\n",
            "iter 40: loss 5.4804, time 31599.22ms, mfu 15.53%\n",
            "iter 41: loss 5.5143, time 31599.06ms, mfu 15.53%\n",
            "iter 42: loss 5.4091, time 31603.90ms, mfu 15.53%\n",
            "iter 43: loss 5.4366, time 31578.38ms, mfu 15.53%\n",
            "iter 44: loss 5.4617, time 31604.10ms, mfu 15.53%\n",
            "iter 45: loss 5.4498, time 31592.73ms, mfu 15.53%\n",
            "iter 46: loss 5.4264, time 31558.46ms, mfu 15.53%\n",
            "iter 47: loss 5.4565, time 31581.94ms, mfu 15.53%\n",
            "iter 48: loss 5.4675, time 31604.01ms, mfu 15.53%\n",
            "iter 49: loss 5.5041, time 31583.19ms, mfu 15.53%\n",
            "iter 50: loss 5.4710, time 31590.57ms, mfu 15.53%\n",
            "iter 51: loss 5.4563, time 31540.60ms, mfu 15.54%\n",
            "iter 52: loss 5.4592, time 31567.65ms, mfu 15.54%\n",
            "iter 53: loss 5.4524, time 31560.70ms, mfu 15.54%\n",
            "iter 54: loss 5.4478, time 31570.10ms, mfu 15.54%\n",
            "iter 55: loss 5.4411, time 31592.72ms, mfu 15.54%\n",
            "iter 56: loss 5.4519, time 31588.78ms, mfu 15.54%\n",
            "iter 57: loss 5.4678, time 31605.99ms, mfu 15.54%\n",
            "iter 58: loss 5.4012, time 31551.59ms, mfu 15.54%\n",
            "iter 59: loss 5.4440, time 31523.49ms, mfu 15.54%\n",
            "iter 60: loss 5.4608, time 31556.19ms, mfu 15.54%\n",
            "iter 61: loss 5.4267, time 31528.30ms, mfu 15.54%\n",
            "iter 62: loss 5.3689, time 31579.75ms, mfu 15.54%\n",
            "iter 63: loss 5.4427, time 31581.38ms, mfu 15.54%\n",
            "iter 64: loss 5.4320, time 31584.77ms, mfu 15.54%\n",
            "iter 65: loss 5.4258, time 31611.31ms, mfu 15.54%\n",
            "iter 66: loss 5.4065, time 31579.87ms, mfu 15.54%\n",
            "iter 67: loss 5.3667, time 31604.46ms, mfu 15.54%\n",
            "iter 68: loss 5.4009, time 31603.03ms, mfu 15.54%\n",
            "iter 69: loss 5.3472, time 31603.06ms, mfu 15.54%\n",
            "iter 70: loss 5.4409, time 31576.84ms, mfu 15.54%\n",
            "iter 71: loss 5.3895, time 31598.67ms, mfu 15.54%\n",
            "iter 72: loss 5.3821, time 31582.83ms, mfu 15.54%\n",
            "iter 73: loss 5.3817, time 31587.14ms, mfu 15.54%\n",
            "iter 74: loss 5.3558, time 31609.51ms, mfu 15.54%\n",
            "iter 75: loss 5.4413, time 31595.11ms, mfu 15.53%\n",
            "iter 76: loss 5.4068, time 31611.07ms, mfu 15.53%\n",
            "iter 77: loss 5.3995, time 31588.24ms, mfu 15.53%\n",
            "iter 78: loss 5.3449, time 31608.35ms, mfu 15.53%\n",
            "iter 79: loss 5.4274, time 31586.73ms, mfu 15.53%\n",
            "iter 80: loss 5.3434, time 31606.06ms, mfu 15.53%\n",
            "iter 81: loss 5.3501, time 31601.76ms, mfu 15.53%\n",
            "iter 82: loss 5.4164, time 31603.49ms, mfu 15.53%\n",
            "iter 83: loss 5.4373, time 31591.22ms, mfu 15.53%\n",
            "iter 84: loss 5.4069, time 31580.59ms, mfu 15.53%\n",
            "iter 85: loss 5.4053, time 31585.92ms, mfu 15.53%\n",
            "iter 86: loss 5.3817, time 31578.94ms, mfu 15.53%\n",
            "iter 87: loss 5.4083, time 31563.87ms, mfu 15.53%\n",
            "iter 88: loss 5.3941, time 31573.96ms, mfu 15.54%\n",
            "iter 89: loss 5.4367, time 31601.19ms, mfu 15.53%\n",
            "iter 90: loss 5.3282, time 31595.97ms, mfu 15.53%\n",
            "iter 91: loss 5.3927, time 31611.86ms, mfu 15.53%\n",
            "iter 92: loss 5.2906, time 31591.21ms, mfu 15.53%\n",
            "iter 93: loss 5.4106, time 31611.71ms, mfu 15.53%\n",
            "iter 94: loss 5.3503, time 31600.24ms, mfu 15.53%\n",
            "iter 95: loss 5.3982, time 31586.07ms, mfu 15.53%\n",
            "iter 96: loss 5.2898, time 31592.05ms, mfu 15.53%\n",
            "iter 97: loss 5.3565, time 31588.66ms, mfu 15.53%\n",
            "iter 98: loss 5.3861, time 31593.10ms, mfu 15.53%\n",
            "iter 99: loss 5.3704, time 31601.87ms, mfu 15.53%\n",
            "iter 100: loss 5.3904, time 31601.94ms, mfu 15.53%\n",
            "iter 101: loss 5.3946, time 31599.73ms, mfu 15.53%\n",
            "iter 102: loss 5.2913, time 31602.23ms, mfu 15.53%\n",
            "iter 103: loss 5.3630, time 31586.96ms, mfu 15.53%\n",
            "iter 104: loss 5.3519, time 31606.69ms, mfu 15.53%\n",
            "iter 105: loss 5.4233, time 31592.84ms, mfu 15.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: since we're training on images it would be nice to sample during training\n",
        "# TODO: We need more powerful GPUs, the T4 in colab is very slow.\n",
        "# TODO: There is no reason to train vanilla attention, we will do a short comparison to demonstrate the gain in throughput when using efficient GPU kernels\n",
        "# TODO: A100 or better should also be able to accommodate max_autotune_gemm mode https://discuss.pytorch.org/t/torch-compile-warning-not-enough-sms-to-use-max-autotune-gemm-mode/184405\n",
        "# TODO: Adjust the positional encoding for image data\n",
        "# TODO: Implement sparse kernels\n",
        "# TODO: Recompute attention and feedforward blocks during backward pass to save memory\n",
        "#   https://pytorch.org/blog/activation-checkpointing-techniques/\n",
        "#   https://docs.pytorch.org/docs/stable/checkpoint.html\n",
        "# TODO: Dropout only applied at the end of each residual addition.\n",
        "# TODO: Pre-activation residual block of https://arxiv.org/pdf/1603.05027\n",
        "# TODO: Weights and biases logging to benchmark memory usage\n",
        "# TODO: Automatic mixed precision\n",
        "#   https://docs.pytorch.org/docs/stable/amp.html\n",
        "\n",
        "# Currently this config peaks at around 11GB GPU HBM usage with torch.compile and no additional checkpointing\n",
        "# TODO: Check impact of 3072 length ground truth\n",
        "#   We should repeat the last pixel twice to avoid cross image contamination\n",
        "\n",
        "# Current model seems to learn, albeit extremely slowly.\n",
        "# Very nice resource\n",
        "#  https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html\n"
      ],
      "metadata": {
        "id": "9MPdFBBBx2yG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
