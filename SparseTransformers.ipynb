{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3F1CgH6GFmP"
      },
      "outputs": [],
      "source": [
        "# Reproducing the Paper step 1:\n",
        "# Training a Vanilla Transformer on CIFAR-10.\n",
        "# It is easier to visualize Attention scores on images, far more intuitive than text data.\n",
        "# We load CIFAR-10 as pixel sequences 32x32x3 = 3072 bytes per image\n",
        "# Text would require byte pair encoding, we already have raw bytes, just need to add an embedding layer\n",
        "# and positional encoding suitable to the data dimensions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A8fS2UOPpmL",
        "outputId": "2cb1dad1-a121-4e35-d6e8-3f3bde943f3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-22 16:17:23--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  30.1MB/s    in 5.9s    \n",
            "\n",
            "2025-08-22 16:17:29 (27.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf cifar-10-python.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTaXLzBoS5rw",
        "outputId": "3cba0071-40cd-4d07-fdd2-f5ca3214949f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as cPickle\n",
        "import numpy as np\n",
        "import random\n",
        "from pathlib import Path\n",
        "import os\n",
        "random.seed(1) # set a seed so that the results are consistent\n",
        "\n",
        "def load_train():\n",
        "    path = Path('./cifar-10-batches-py')\n",
        "    filestring = 'data_batch_'\n",
        "    contents = [path / x for x in os.listdir(path) if filestring in x]\n",
        "\n",
        "    images = [cPickle.load(open(f, \"rb\"), encoding=\"latin1\") for f in contents]\n",
        "\n",
        "    images = [x[\"data\"] for x in images]\n",
        "    imagearray = np.array(images)   #   (5, 10000, 3072)\n",
        "    return np.vstack(imagearray) # (50000, 3072)\n",
        "\n",
        "def load_test():\n",
        "    path = Path('./cifar-10-batches-py')\n",
        "    filestring = 'test_batch'\n",
        "    contents = [path / x for x in os.listdir(path) if filestring in x]\n",
        "\n",
        "    images = [cPickle.load(open(f, \"rb\"), encoding=\"latin1\") for f in contents]\n",
        "\n",
        "    images = [x[\"data\"] for x in images]\n",
        "    imagearray = np.array(images)   #   (5, 10000, 3072)\n",
        "    return np.vstack(imagearray) # (50000, 3072)\n"
      ],
      "metadata": {
        "id": "imPYK-CYT2zN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = load_train()\n",
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBFNkfowUHYb",
        "outputId": "89c6b345-474f-46b4-923d-4f07253a9216"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 3072)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = load_test()\n",
        "imgs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIqY0LU8lAKw",
        "outputId": "2eac9290-6639-415b-f3f2-61bc5136526d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3072)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's save one of the images to disk to verify integrity\n",
        "images = np.reshape(images, (50000, 3, 32, 32))\n",
        "img = images[1000]\n",
        "img = np.transpose(img, (1, 2, 0))\n",
        "img.shape\n",
        "import matplotlib\n",
        "matplotlib.pyplot.imsave('img.jpg', img)     # image no #"
      ],
      "metadata": {
        "id": "dOZyrFeHXVyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = load_train()\n",
        "bytesequence = images[0]\n",
        "bytesequence[0:100]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKK03cL8UE3m",
        "outputId": "f1546bff-9b10-4425-a979-3f6b2adff264"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([26, 17, 13, 13, 13, 14, 14, 15, 14, 10, 13, 26, 18, 11, 10, 12, 10,\n",
              "        9, 19, 22, 17, 22, 15, 19, 12, 21, 23, 17, 17, 15, 24, 22, 20, 13,\n",
              "       13, 11, 11, 12, 11, 22, 22, 12, 13, 14, 13, 10, 12, 15,  8, 11, 40,\n",
              "       21,  9, 16, 17, 25,  9, 14, 16, 23, 36, 19, 21, 29, 14, 13, 13, 14,\n",
              "       11, 10, 13, 29, 21, 11, 12, 16, 20,  8, 12, 13,  8, 13, 36, 13,  9,\n",
              "       10,  7, 12, 15, 21, 36, 26, 22, 17, 25, 31, 13, 17, 14, 14],\n",
              "      dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's implement our vanilla attention baseline\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "0hOPIyA834CQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  \"\"\"LayerNorm with optional bias\"\"\"\n",
        "  def __init__(self, ndim, bias):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.ones(ndim))\n",
        "    self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "  def forward(self, input):\n",
        "    return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "lYmp_yez4RSI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    # key, query, value projections for all heads, but in a batch\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "    # output projection\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "    # regularization\n",
        "    self.attn_dropout = nn.Dropout(config.dropout)\n",
        "    self.resid_dropout = nn.Dropout(config.dropout)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.dropout = config.dropout\n",
        "    # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()\n",
        "    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "    q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    att = self.attn_dropout(att)\n",
        "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "    # output projection\n",
        "    y = self.resid_dropout(self.c_proj(y))\n",
        "    return y"
      ],
      "metadata": {
        "id": "TrDKszBz46R4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7ONXRImt6PLw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "JaRDCZiS6Tey"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 3072\n",
        "    vocab_size: int = 256 # 256 possible byte values\n",
        "    n_layer: int = 128\n",
        "    n_head: int = 2\n",
        "    n_embd: int = 256\n",
        "    dropout: float = 0.25\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n"
      ],
      "metadata": {
        "id": "Gxa3o4ti6Vpz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:  mlp and query-key projections to half-size"
      ],
      "metadata": {
        "id": "ivaefOGX7YGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        pass\n",
        "        # TODO: Implement for easy checkpointing\n",
        "        # return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Zhbywm7i7jqi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now it's time to convert our training and validation data to .bin files and\n",
        "# set up the training loop"
      ],
      "metadata": {
        "id": "93LrVJid99-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we save the train and test data to .bin files to conform with the rest of the transformer implementation\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "train_data = load_train()\n",
        "val_data = load_test()\n",
        "# data is already encoded as raw bytes\n",
        "\n",
        "# export to bin files\n",
        "\n",
        "train_data.tofile('train.bin')\n",
        "val_data.tofile('val.bin')\n"
      ],
      "metadata": {
        "id": "G9qB6kTX-e-5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, val_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQdsmEkElfzb",
        "outputId": "d09d4c92-b854-4c96-fb47-39a6378c8b22"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 3072), (10000, 3072))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir out/"
      ],
      "metadata": {
        "id": "mjFKZLaJnn5v"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group"
      ],
      "metadata": {
        "id": "SH48eP79mrKf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 2000\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "\n",
        "# TODO: set up wandb logging\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'sparse-transformer'\n",
        "wandb_run_name = 'cifar-10' # 'run' + str(time.time())\n",
        "\n",
        "# data\n",
        "dataset = 'cifar-10'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 3072\n",
        "\n",
        "# model\n",
        "# For testing purposes\n",
        "n_layer = 12\n",
        "n_head = 4\n",
        "n_embd = 256\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "\n",
        "# for actual training:\n",
        "# block_size: int = 3072\n",
        "# vocab_size: int = 256 # 256 possible byte values\n",
        "# n_layer: int = 128\n",
        "# n_head: int = 2\n",
        "# n_embd: int = 256\n",
        "# dropout: float = 0.25\n",
        "# bias: bool = True\n",
        "\n",
        "# adamw optimizer\n",
        "learning_rate = 0.00035 # max learning rate\n",
        "# 120 epochs of 50k images = 6 million iterations / batch size = 500k iters\n",
        "max_iters = 500000 # total number of training iterations\n",
        "weight_decay = 0.01\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "# gradient clipping as specified in paper\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 5000 # how many steps to warm up for\n",
        "lr_decay_iters = 500000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 0.000035 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster"
      ],
      "metadata": {
        "id": "BhZy4_7Nn9mV"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "# TODO: implement distributed training"
      ],
      "metadata": {
        "id": "CdzXH9WIshFf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if ddp:\n",
        "    pass\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1"
      ],
      "metadata": {
        "id": "3qrxBPQIsoLB"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLZ5nHxEswms",
        "outputId": "55f3a513-8228-41c1-cc2a-f449d3cab72a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 491,520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRbWR67Us9Km",
        "outputId": "cf9aa59d-e353-4e64-d6a7-227def284c59"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"data_dir\", exist_ok=True)"
      ],
      "metadata": {
        "id": "UWRWgNh2tD9g"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv train.bin data_dir/\n",
        "!mv val.bin data_dir/"
      ],
      "metadata": {
        "id": "tSwg8OsZtL0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(\"data_dir\", 'train.bin'), dtype=np.uint8, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(\"data_dir\", 'val.bin'), dtype=np.uint8, mode='r')\n",
        "    # divide by block_size since we treat images as discrete samples\n",
        "    ix = torch.randint(len(data)//block_size - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "OfVt_DcutkVo"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.memmap(os.path.join(\"data_dir\", 'train.bin'), dtype=np.uint8, mode='r')\n",
        "data.shape\n",
        "# data is written to disk as one dimensional array of bytes. We need to sample\n",
        "# chunks with offsets 3072*n\n",
        "ix = torch.randint(len(data)//block_size - block_size, (batch_size,))\n",
        "x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE5hPjXbt2vC",
        "outputId": "61d2f80f-ad91-46a5-fc07-0b467ac18c59"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: technically we need to remove one byte. how much does this influence the\n",
        "y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn8EJ4WivREZ",
        "outputId": "3bddd409-e8d7-49d3-fa49-ab833181a39d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = get_batch(\"train\")\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "huH5lDfyt-4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "meta_vocab_size = 256"
      ],
      "metadata": {
        "id": "mUA9Os5bu2cy"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line"
      ],
      "metadata": {
        "id": "pvRXjIyzwyoc"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 256\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hDCC5F3w6Bm",
        "outputId": "8018a04a-feef-409e-a0e0-67264c496b08"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 9.51M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "b-sJgzXxxDii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "# TODO: implement model.configure_optimizers\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory\n"
      ],
      "metadata": {
        "id": "O3W_d_9xxJXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n"
      ],
      "metadata": {
        "id": "sHvV4SD4xXIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "O2GOrCUMxexV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * (it + 1) / (warmup_iters + 1)\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "ITiX7150xhiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n"
      ],
      "metadata": {
        "id": "Wbi1hnhdxnid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "id": "2U4YEeO8xr8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: since we're training on images it would be nice to sample during training"
      ],
      "metadata": {
        "id": "9MPdFBBBx2yG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}